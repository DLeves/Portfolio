---
title: "Budapesti lakásárak elemzése"
author: "Dittrich Levente"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ebben a portfolió fejezetben azt vizsgálom meg, hogy az [AMD](https://finance.yahoo.com/quote/AMD?p=AMD&.tsrc=fin-srch), [NVIDIA](https://finance.yahoo.com/quote/NVDA?p=NVDA&.tsrc=fin-srch) és [Intel](https://finance.yahoo.com/quote/INTC?p=INTC&.tsrc=fin-srch) vállalatok részvényeinak árfolyamára fogok VECM vagy VAR modellt illeszteni, attól függően, hogy van-e közös hosszútávú pályájuk.

Mindhárom vállalat székhelye a kaliforniai Santa Claraban található, azonban nem csak az a közös bennük, hanem a tevékenységük is hasonló. Ezek a vállalatok mind mikro- és grafikus proceszorokkal foglalkoznak elsősorban. A napi gyakoriságú részvényadatokat a Yahoo Finance-ről töltöm le, viszont egy meghatározott intervallumban, így reprodukálhatóak lesznek eredményeim.

# Kezdeti beállítások

## Használt package-ek

```{r warning=FALSE, message=FALSE}
library(quantmod)
library(tidyverse)
library(knitr)
library(urca)
library(lmtest)
library(aTSA)
library(vars)
library(tsDyn)
library(aod)
```

Egy részvénynek egy napra több értékei is lehet, például az adott napi maximum, minimum, a nyitó vagy záró értéke. A továbbiakban az egyes részvények záró értékeivel fogok dolgozni.

## Adatok megszerzése

```{r}
intel = getSymbols("INTC", src = "yahoo", auto.assign = F, from = "2010-01-01", to = "2023-06-01")
amd = getSymbols("AMD", src = "yahoo", auto.assign = F, from = "2010-01-01", to = "2023-06-01")
nvidia = getSymbols("NVDA", src = "yahoo", auto.assign = F, from = "2010-01-01", to = "2023-06-01")
```

## Adatok átalakítása

Érdemes az adatokat nem xts formátumban egyenként, hanem dataframe-ként egyben tárolni, hogy később könnyebb legyen a munka.

```{r}
df = data.frame(
  time = index(intel),
  intc = as.numeric(intel$INTC.Close),
  amd = as.numeric(amd$AMD.Close),
  nvda = as.numeric(nvidia$NVDA.Close)
)
rm(intel, amd, nvidia)
kable(head(df))
```

Az adatokat tartalmazó dataframe változói a következők:

| Változó neve | Leírás                                    | Mértékegység |
|:-------------|:------------------------------------------|:-------------|
| time         | Idő(az adott nap)                         | Dátum        |
| intc         | Az adott napi Intel részvény záró értéke  | \$           |
| amd          | Az adott napi AMD részvény záró értéke    | \$           |
| nvda         | Az adott napi NVIDIA részvény záró értéke | \$           |

3375 megfigyelés van mindhárom változóból, ez mindenképpen elegendő lesz a modellépítéshez.

# Adatvizualizáció

```{r}
ggplot(df, aes(x = time))+
  geom_line(aes(y = intc, col = "Intel"))+
  geom_line(aes(y = amd, col = "AMD"))+
  geom_line(aes(y = nvda, col = "NVIDIA"))+
  scale_y_continuous(labels = scales::dollar_format())+
  scale_x_date(date_breaks = "years", date_labels = "%y")+
  scale_color_manual(values = c("#ED1C24", "#0071C5", "#76B900"))+
  theme_minimal()+
  theme(legend.title = element_blank())+
  labs(x = "Idő", y = "Részvény árfolyam")
```

Már a grafikonon is látszik, hogy valószínűleg nem stacionerek az idősorok. Látható, hogy a kezdetben az NVIDIA és az AMD együtt mozgott, ahogy az Intel is magasabb pályán, de hasonlóan. Ezt követően körülbelül 2015 Q4-től erőteljes emelkedésbe kezdett az NVIDIA, amit az AMD nagyrészt lekövetett, azonban korántsem olyan meredeken. Az Intel is növekedett ez idő alatt, de nem tűnik úgy, mintha együtt mozgott volna a másik két céggel. Ez lehet akár azért is, mert ezidő tájt kezdődött el a kriptobányászat már terjengeni a levegőben (de csak nagyon szubkultúrális szinten ekkor még, [2017 decemerétől lesz világszinten felkapott](https://trends.google.com/trends/explore?date=all&q=%2Fm%2F05p0rrx&hl=hu)), ami nagyon videókártya-igényes tevékenység. Mindhárom cégnél 2021 végén volt egy csúcspot, ami után csökkenni kezdett az árfolyamuk, majd 2022 Q4-től megint emelkedés látszott, az NVIDIA árfolyama a 2021-es csúcsát is túlszárnyalta.

# Kointegréció tesztelése

Ahhoz, hogy tudjam, hogy VEC(Vector Error Correction) vagy VAR(Vector AutoRegression) modellt kell-e használjak, meg kell állapítanom, hogy van-e közös hosszútávú pályájuk az idősoroknak. Kointegráció esetén VEC modell a megfelelő, míg ha nincs közös távú hosszútávú pályája az idósoroknak, akkor VAR modellt kell alkalmazzak.

Erre egy *Johansen-tesztet* fogok végezni. A teszt érdekessége, hogy több nullhipotézist is felvet. Ez esetben a nullhipotézisek a következők, melyekben *r* a közös hosszútávú pályák száma:

-   H0: $r = 0$ \| H1: $r > 0$
-   H0: $r \le 1$ \| H1: $r > 1$
-   H0: $r \le 2$ \| H1: $r > 2$

Mivel összesen 3 db idősorom van, ezért maximum 2 db közös hosszútávú pálya lehetséges (a kódban ez a *K* paraméter).

```{r}
johansen_test = ca.jo(df[,-1], type = "eigen",  K = 2, ecdet = "const", spec = "longrun")
summary(johansen_test)
```

A teszt egy jobboldali próba, ez azt jelenti, hogy akkor tudjuk elfogadni a nullhipotéziseket, ha az adott próbafüggvény kisebb vagy egyenlő a kritikus értéknél. $H0: r = 0$ Minden szokványos szignifikanciaszinten elfogadható. $H0: r \le 1$ Minden szokványos szignifikanciaszinten elfogadható. $H0: r \le 2$ Minden szokványos szignifikanciaszinten elfogadható.

Ilyen esetben én a kettő közös hosszútávú pályát venném kiindulópontnak, amennyiben nem lesz szignifikáns ECT2, akkor megfontolom az egy hosszútávú pályára váltást.

# Optimális késleltetés megtalálása

Meg kellene határozzam az optimális késleltetést a VEC modellhez. Ez több lépésben fog megtörténni, először az idősorok stacionaritását kell tesztelnem. Amennyiben van egységgyök az idősorban, akkor stacionerré kell alakítanom őket majd mintha VAR modellhez keresném, meg kell határozzam az optimális késleltetést az információs kritériumokat felhasználva.

## Stacionaritás

Az erős stacionaritás azt jelenti, hogy az idősorok minden véges dimenziós eloszlása eltolásinvariás. Ez egy olyan erős követelmény, amit inkább a valószínűségszámításban használnak gyakran, azonban ökonometriában túl szigorú követelmény, ezért szokás gyenge stacionaritással dolgozni. Ez azt jelenti, hogy az idősor szórása és várható értéke időben állandó. A stacionaritás teszteléséhez egy Augmented Dickey-Fuller tesztet fogok végezni.

Az ADF teszt hipotézisei:

-   H0: Az idősor nem stacioner, $\phi = 0$
-   H1: Az idősor stacionárius, $\phi \neq 0$

A teszt nem csak egyféle módon végzi a tesztet, random-walkkal, valamint random walkkal és trenddel is megnézi a stacionaritást.

```{r}
adf.test(df$intc)
adf.test(df$amd)
adf.test(df$nvda)
```

Egyik idősor esetében sem lehet elutasítani a nullhipotézist, minden idősorom stacionárius.

```{r}
adf.test(diff(df$intc))
adf.test(diff(df$amd))
adf.test(diff(df$nvda))
```

A differenciált idősorok már stacionerek, nincsen egységgyök a modellben. Minden esetben a p-értékek 1% alatt vannak, a nullhipotéziseket el lehet vetni.

## Késleltetés meghatározása

A differenciált idősorokat a dataframe-be lementem és így nézem meg az optimális lag-okat.

```{r}
df$d_intc = c(NA, diff(df$intc))
df$d_amd = c(NA, diff(df$amd))
df$d_nvda = c(NA, diff(df$nvda))

VARselect(df[-1,5:7], lag.max = 30)
```

Mind az AIC, mind az FPE nagyon nagy lag-okat javasol. Ezeknél az információs kritériumoknál szigorúbb Hannan-Quinn információs kritérium a 11-es késleltetést preferálja, míg az annál is szigorúbb Bayes-Schwartz értéke az 1-es késleltetésnél a legkisebb. Ebben az esetben a Hannan-Quinn infromációs kritérium szerint fogok dönteni, 11-es késleletés lesz az alapmodellemben a kettő darab hosszútávú közös pálya mellett.

# Modellépítés

## Alapmodell

```{r}
alapmodell = VECM(df[,2:4], lag = 11, r = 2)
summary(alapmodell)
```

A közös hosszútávú pálya koefficiensei csak az AMD esetében szignifikánsak. Ezek jelentése a következő az AMD esetében:

-   ECT1: amennyiben az AMD elmozdul az első közös hosszútávú pályától, akkor a következő időszakban 0,51%-al a távolodik még.
-   ECT2: amennyiben az AMD elmozdul a második közös hosszútávú pályától, akkor az elmozdulás 0,63%-át hozza be a következő időszakban.

Érdemes kiemelni a kontextus miatt, hogy az Intel a x86 CPU-k piacának nagy részét az [Intel dominálja](https://www.statista.com/statistics/735904/worldwide-x86-intel-amd-market-share/), míg a diszkrét GPU-k piacának nagy részét az [NVIDIA uralja](https://www.pcworld.com/article/1526632/intel-is-already-tied-with-amd-for-desktop-gpu-sales.html). A laptopokat is beleértve szintén az [Intel a legnagyobb szereplő](https://www.statista.com/statistics/754557/worldwide-gpu-shipments-market-share-by-vendor/) a GPU piacon. Azért érdemes külön választani a diszkrét és integrált GPU-k piacát, mivel a 2010-es évek végétől kezdődő kriptovaluta bányászatra elsősorban dedikált, asztali videókártyákat használnak, ami egy időben okozott is hiányt, mivel a bányászok rengeteg GPU-t vásároltak fel a piacról.

Valószínűnek tartom, hogy a két hosszútávú közös pálya a CPU és GPU piacokat jelenti. Ha a szignifikanciaszinttől függetlenül nézzük a pályák koefficienseit, akkor látható, hogy az első pályától való eltérésnél az Intel közeledik, míg az AMD és az NVIDIA távolodik a következő időszakban. A második hosszútávú közös pályát nézve pont fordított a helyzet: az eltérés hatására az Intel távolodik, míg az AMD és az NVIDIA ledolgozza az eltérés egy részét. Feltevésem szerint az első hosszútávú pálya a processzorgyártók hosszútávú pályája, míg a második közös hosszútávú pálya a GPU gyártóké.

Visszatérve az alapmodellre, nagyon érdekes, hogy az előző időszaki Intel árfolyam mindegyik részvényre negatív hatással van. Végig a 11 késleltetésig nagyrészt vannak szignifikáns együtthatók, egyedül a négyes késleltetésnél nincsen egyik egyenletben sem szingifikáns koefficiens.

## Modellszűkítés

A modellszelekció miatt megnéztem több, más paraméterű modellt, ezek közül kettőt emelnék ki:

Az egyik modellben egy közös hosszútávú pálya van 11 késleltetéssel:

```{r}
szukitett_modell1 = VECM(df[,2:4], lag = 11, r = 1)
summary(szukitett_modell1)
```

A másik modellben pedig megmaradt a kettő közös hosszútávú pálya, azonban a késleltetést a Bayes-Schwartz IC által preferált 1-es késleltetésre cseréltem.

```{r}
szukitett_modell2 = VECM(df[,2:4], lag = 1, r = 2)
summary(szukitett_modell2)
```

A modellek információs kritériumai:

| Modell      | ECT | Lag | AIC               | BIC        |
|:------------|:----|:----|:------------------|:-----------|
| Alap        | 2   | 11  | 6161.923 \*       | 6835.188   |
| Szűkített 1 | 1   | 11  | 6167.099          | 6822.002   |
| Szűkített 2 | 2   | 1   | 6467.342          | 6589.813 \*|
|             |     |     | \* *Legkisebb IC* |            |

Az AIC szerint az alapmodell a jobb, míg a BIC szerint az egy késleltetésű, ami érthető is, mivel akkor drasztikusan csökken a bevont változók száma. Mivel egyértelműen nem tudom eldönteni, hogy szűkítsem-e a modellt, ezért amellett döntök, hogy meghagyom az alapmodellt végleges modellnek, mert egészen a 11 lagig vannak szignifikáns magyarázó változók.

## Stabilitás tesztelése

A modell staibilitásának tesztelésére először megvizsgálom, hogy a hibatagok fehérzajok-e, majd azt is megnézem, hogy normális eloszlásúak-e.

A Breusch-Godfrey teszt nullhipotézise szerint az $\hat{u_t}$-re felírt segédregresszóból $R^2 = 0$, vagyis a VECM egyenleteinek hibatagjai fehérzajok.
```{r}
lapply(as.data.frame(resid(alapmodell)), function(i) bgtest(i ~ 1, order = 11))
```
Nagyon magasak a p-értékek, a nullhipotézist nem lehet elvetni egyik esetben sem.

A hibatagok normális eloszlásának is megvizsgálatához egy Shapiro-Wilk tesztet fogok alkalmazni. A teszt hipotézisei:

-   H0: a hibatagok normális eloszlásúak
-   H1: a hibatagok nem normális eloszlásúak

```{r}
lapply(as.data.frame(resid(alapmodell)), function(i) shapiro.test(i))
```
Mindegyik tesztben nagyon alacsony p-értékek jöttek ki, H0-t el lehet vetni még 1%-os szignifikancia szint mellett is. Egyik egyenlet hibatagja sem normális eloszlású.

# Granger-okság

A Granger-okság két idősor kapcsolatát vizsgálja, definíció szerint $X$ idősor Granger okozza $Y$ idősort, ha $X$ valamely múltja szignifikáns hatéssal van $Y$ aktuális értékére.

A VEC modelleknél a Granger okság vizsgálata bonyolultabb, mint a VAR modelleknél. Először is meg kell keressem, hogy az egyes egyenletek a koefficinsek és a variancia-kovariancia táblázatban hol helyezkednek el.

```{r}
coef = coef(alapmodell)
knitr::kable(coef, format = "html")
```

Az első egyenelet az intc, majd az amd és az nvda követi.

```{r}
vcov = vcov(alapmodell)
```

A Variancia-kovariancia táblázatban a [1:36,1:36] helyen az intc, [37:72,37:72] helyen az amd és végül a [73:108,73:108] helyen az nvda egyenlete van. A táblázatot nem szándékozom megjeleníteni, mivel feleslegesen sok helyet foglalna el, azonban a Githubon található `.rmd` futtatásával meg lehet tekinteni. 

A Granger okság szerint az az okozó változó, ami időben előbb van. A Granger-okság tesztelésére egy Wald-tesztet fogok használni.

-   A teszt nullhipotézise szerint $X$ idősor nem Granger okozza $Y$ idősort, $X$ idősor lagjai nem magyarázzák $Y$ idősor aktuális értékét.
-   Az alternatív hipotézis szerint az $X$ idősor Granger okozza $Y$ idősort.

Mivel a változók közötti Granger-okságot kell megállapítsam, ezért 3 változónál, 2 irányban ezért összesen 6 db Wald-tesztet kell végezzek.

```{r}
# amd -> intel
wald.test(b = coef[1,],
          Sigma = vcov[1:36,1:36],
          Terms = c(5,8,11,14,17,20,23,26,29,32,35))

# amd -> nvidia
wald.test(b = coef[3,],
          Sigma = vcov[73:108,73:108],
          Terms = c(5,8,11,14,17,20,23,26,29,32,35))

# intel -> amd
wald.test(b = coef[2,],
          Sigma = vcov[37:72,37:72],
          Terms = c(4,7,10,13,16,19,22,25,28,31,34))

# intel -> nvidia
wald.test(b = coef[3,],
          Sigma = vcov[73:108,73:108],
          Terms = c(4,7,10,13,16,19,22,25,28,31,34))

# nvidia -> amd
wald.test(b = coef[2,],
          Sigma = vcov[37:72,37:72],
          Terms = c(6,9,12,15,18,21,24,27,30,33,36))

# nvidia -> intel
wald.test(b = coef[1,],
          Sigma = vcov[1:36,1:36],
          Terms = c(6,9,12,15,18,21,24,27,30,33,36))

```

Mindegyik egyenlet 1% alatt szignifikáns, el lehet vetni a nullhipotézist minden esetben. Ez azt jelenti, hogy mindegyik idősor Grager-okozza a másik kettőt. Ez a Granger-okság egy különös esete, mivel az egyes idősorok késleltetései minden esetben magyarázzák a más idősorok aktuális értékét.

# Impulzus-válaszfüggvények

Az impulzus-válaszfüggvények azt mutatják meg, hogy az egységnyi szórásnyi sokk az egyik idősoron, akkor az hogyan cseng le önmagában és az összes többi változóban. 

```{r}
plot(irf(alapmodell, impulse = "intl", response = "amd", n.ahead = 30, ortho = F))
```

Az intc-ben bekövetkező sokk az amd-ben és az nvda-ban is szignifikáns csökkenést okoz az első, illetve az 5-6. késleltetéstől minimum 30 lagig.

Az amd-ben bekövetkező sokk a 3-6. késleletésben minimális szingifikáns csökkenést okoz, azonban a többi időszakban a 95%-os konfidenci intervallumba beleesik a nulla, így ott feltehetőleg nincsen hatása. Az nvda esetében a 8-9. lagban van szignifikáns növekedés az egységnyi amd-ben bekövetkező sokk hatására, majd szintén nem szignifikáns a hatás 95%-on.

Az nvda-ban bekövetkező sokknak nincsen szingnifikáns hatása az intc-re és amd-re.

# Becslési variancia dekompozíció

A becslési varianci dekompozíció azt mutatja meg, hogy az adott eredményváltozónak a standrad becslési hibájának négyzetét mekkora részben lehet magyarázni a modell többi változójával. 
```{r}
fevd(alapmodell, n.ahead = 11)
```

Az intc becslési varianciájának ~99%-át lehet magyarázni a saját késleltetésével, az amd és nvda elhanyagolható mértékben járul hozzá ehhez a 11 késleltetésen keresztül.

Az amd becslési varianciájának 88-93%-át lehet magyarázni a saját lagjával, a maradék 12-6%-ot az intel késleltetéseival, az nvda itt is elhanyagolható mértékben szerepel.

Az nvda becslési varianciáját az első késleltetésben 11%-ban az intel, 46%-ban az amd és 41%-ban önmaga magyarázza. A további időszakokban az amd magyarázó ereje tovább nő, egészen majdnem 53%-ig, az intc lecsökken 6%-ra, illetve az nvda is lecsökken minimálisan, 40%-ra.
